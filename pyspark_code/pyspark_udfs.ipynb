{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------------+---------+--------+--------+\n",
      "|      date|time|         province|confirmed|released|deceased|\n",
      "+----------+----+-----------------+---------+--------+--------+\n",
      "|2020-01-20|  16|            Seoul|        0|       0|       0|\n",
      "|2020-01-20|  16|            Busan|        0|       0|       0|\n",
      "|2020-01-20|  16|            Daegu|        0|       0|       0|\n",
      "|2020-01-20|  16|          Incheon|        1|       0|       0|\n",
      "|2020-01-20|  16|          Gwangju|        0|       0|       0|\n",
      "|2020-01-20|  16|          Daejeon|        0|       0|       0|\n",
      "|2020-01-20|  16|            Ulsan|        0|       0|       0|\n",
      "|2020-01-20|  16|           Sejong|        0|       0|       0|\n",
      "|2020-01-20|  16|      Gyeonggi-do|        0|       0|       0|\n",
      "|2020-01-20|  16|       Gangwon-do|        0|       0|       0|\n",
      "|2020-01-20|  16|Chungcheongbuk-do|        0|       0|       0|\n",
      "|2020-01-20|  16|Chungcheongnam-do|        0|       0|       0|\n",
      "|2020-01-20|  16|     Jeollabuk-do|        0|       0|       0|\n",
      "|2020-01-20|  16|     Jeollanam-do|        0|       0|       0|\n",
      "|2020-01-20|  16| Gyeongsangbuk-do|        0|       0|       0|\n",
      "|2020-01-20|  16| Gyeongsangnam-do|        0|       0|       0|\n",
      "|2020-01-20|  16|          Jeju-do|        0|       0|       0|\n",
      "|2020-01-21|  16|            Seoul|        0|       0|       0|\n",
      "|2020-01-21|  16|            Busan|        0|       0|       0|\n",
      "|2020-01-21|  16|            Daegu|        0|       0|       0|\n",
      "+----------+----+-----------------+---------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('TimeProvince.csv', header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- confirmed: string (nullable = true)\n",
      " |-- released: string (nullable = true)\n",
      " |-- deceased: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toint(x):\n",
    "    if isinstance(x, str) == True:\n",
    "        x1 = [str(ord(i)) for i in x]\n",
    "        return (int(''.join(x)))\n",
    "    else:\n",
    "        return Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "col_int = udf(lambda z: toint(z), IntegerType())\n",
    "spark.udf.register(\"col_int\", col_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------------+---------+--------+--------+----------+\n",
      "|      date|time|         province|confirmed|released|deceased|confirmed1|\n",
      "+----------+----+-----------------+---------+--------+--------+----------+\n",
      "|2020-01-20|  16|            Seoul|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|            Busan|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|            Daegu|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|          Incheon|        1|       0|       0|         1|\n",
      "|2020-01-20|  16|          Gwangju|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|          Daejeon|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|            Ulsan|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|           Sejong|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|      Gyeonggi-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|       Gangwon-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|Chungcheongbuk-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|Chungcheongnam-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|     Jeollabuk-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|     Jeollanam-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16| Gyeongsangbuk-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16| Gyeongsangnam-do|        0|       0|       0|         0|\n",
      "|2020-01-20|  16|          Jeju-do|        0|       0|       0|         0|\n",
      "|2020-01-21|  16|            Seoul|        0|       0|       0|         0|\n",
      "|2020-01-21|  16|            Busan|        0|       0|       0|         0|\n",
      "|2020-01-21|  16|            Daegu|        0|       0|       0|         0|\n",
      "+----------+----+-----------------+---------+--------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('confirmed1', col_int('confirmed'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- confirmed: string (nullable = true)\n",
      " |-- released: string (nullable = true)\n",
      " |-- deceased: string (nullable = true)\n",
      " |-- confirmed1: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def to_upper(s):\n",
    "    if s is not None:\n",
    "        return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|province|confirmed1|\n",
      "+--------+----------+\n",
      "|   SEOUL|         0|\n",
      "|   BUSAN|         0|\n",
      "|   DAEGU|         0|\n",
      "| INCHEON|         1|\n",
      "| GWANGJU|         0|\n",
      "+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(to_upper('province').alias('province'), 'confirmed1').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_upper = udf(lambda z: to_upper(z), StringType())\n",
    "# spark.udf.register(\"to_upper\", to_upper)\n",
    "# df1.withColumn('province1', to_upper('province')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+-----+-------+---------+-------+-------+---------+-------+------+\n",
      "|     _c0|Gender|       DOB|Maths|Physics|Chemistry|English|Biology|Economics|History|Civics|\n",
      "+--------+------+----------+-----+-------+---------+-------+-------+---------+-------+------+\n",
      "|    John|     M|05/04/1988|   55|     45|       56|     87|     21|       52|     89|    65|\n",
      "|  Suresh|     M|  4/5/1987|   75|     55|     null|     64|     90|       61|     58|     2|\n",
      "|  Ramesh|     M| 25/5/1989|   25|     54|       89|     76|     95|       87|     56|    74|\n",
      "| Jessica|     F| 12/8/1990|   78|     55|       86|     63|     54|       89|     75|    45|\n",
      "|Jennifer|     F|  2/9/1989|   58|     96|       78|     46|     96|       77|     83|    53|\n",
      "+--------+------+----------+-----+-------+---------+-------+-------+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.csv('student_marks.csv', header=True)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- DOB: string (nullable = true)\n",
      " |-- Maths: string (nullable = true)\n",
      " |-- Physics: string (nullable = true)\n",
      " |-- Chemistry: string (nullable = true)\n",
      " |-- English: string (nullable = true)\n",
      " |-- Biology: string (nullable = true)\n",
      " |-- Economics: string (nullable = true)\n",
      " |-- History: string (nullable = true)\n",
      " |-- Civics: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+-----+-------+---------+-------+-------+---------+-------+------+------+\n",
      "|     _c0|Gender|       DOB|Maths|Physics|Chemistry|English|Biology|Economics|History|Civics|result|\n",
      "+--------+------+----------+-----+-------+---------+-------+-------+---------+-------+------+------+\n",
      "|    John|     M|05/04/1988|   55|     45|       56|     87|     21|       52|     89|    65|  Pass|\n",
      "|  Suresh|     M|  4/5/1987|   75|     55|     null|     64|     90|       61|     58|     2|  Pass|\n",
      "|  Ramesh|     M| 25/5/1989|   25|     54|       89|     76|     95|       87|     56|    74|  fail|\n",
      "| Jessica|     F| 12/8/1990|   78|     55|       86|     63|     54|       89|     75|    45|  Pass|\n",
      "|Jennifer|     F|  2/9/1989|   58|     96|       78|     46|     96|       77|     83|    53|  Pass|\n",
      "+--------+------+----------+-----+-------+---------+-------+-------+---------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_pass_or_fail(x):\n",
    "    if x > '50':\n",
    "        return \"Pass\"\n",
    "    else:\n",
    "        return \"fail\"\n",
    "result = udf(lambda z: get_pass_or_fail(z), StringType())\n",
    "spark.udf.register('get_pass_or_fail', get_pass_or_fail)\n",
    "df4 = df3.withColumn('result', result('Maths'))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- DOB: string (nullable = true)\n",
      " |-- Maths: string (nullable = true)\n",
      " |-- Physics: string (nullable = true)\n",
      " |-- Chemistry: string (nullable = true)\n",
      " |-- English: string (nullable = true)\n",
      " |-- Biology: string (nullable = true)\n",
      " |-- Economics: string (nullable = true)\n",
      " |-- History: string (nullable = true)\n",
      " |-- Civics: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(x):\n",
    "#     return x.count('province')\n",
    "# result = udf(lambda z: get_data(z), IntegerType())\n",
    "# spark.udf.register('get_data', get_data)\n",
    "# res = df.withColumn('result', result('province'))\n",
    "# res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|name_length|result|\n",
      "+-----------+------+\n",
      "|          4|  Pass|\n",
      "|          6|  Pass|\n",
      "|          6|  fail|\n",
      "|          7|  Pass|\n",
      "|          8|  Pass|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "df4 = df4.select(slen('_c0').alias('name_length'), 'result')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
